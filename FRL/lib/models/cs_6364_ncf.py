# -*- coding: utf-8 -*-
"""CS_6364_NCF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kq0KEclj5uhnucCZYcOMOxYXn7DcOuVg
"""

import os
import time
import argparse
import pandas as pd
import numpy as np
import random

import torch
import torch.nn as nn

import pickle
import zipfile
import math

from google.colab import drive
drive.mount('/content/drive')

###Execute this cell only if the tar file isn't processed.
zip_file_path = '/content/drive/MyDrive/frl/processed_rating.tar.zip'

zip_ref = zipfile.ZipFile(zip_file_path, 'r')
zip_ref.extractall()
zip_ref.close()

!tar -xvf "processed_rating.tar"

root_path = "/content/processed/"
rating_file_path = list()
for files in os.listdir(root_path):
  file_name = os.path.join(root_path, files)
  rating_file_path.append(file_name)

print(rating_file_path)

def get_rating_df(rating_file_path):
  for i in range(0, len(rating_file_path)):
    #print(i)
    with open(rating_file_path[3], 'rb') as f:
      #print(rating_file_path[i])
      rating_df = pickle.load(f)
    
    break

  df = pd.DataFrame(rating_df, columns=['user', 'song_item', 'rating'])
    # if i ==0:
    #   final_df = df
    #   break
    # else:
    #   final_df = pd.concat([final_df,df])
      #final_df.concat(df)

  #print(len(final_df))

  return df

final_df = get_rating_df(rating_file_path)
len(final_df)

parser = argparse.ArgumentParser()
parser.add_argument("--lr", type=float, default=0.001, help="learning rate")
parser.add_argument("--dropout", type=float,default=0.2,  help="dropout rate")
parser.add_argument("--batch_size", type=int, default=256, help="batch size for training")
parser.add_argument("--epochs", type=int,default=4,  help="training epoches")
parser.add_argument("--top_k", type=int, default=10, help="compute metrics@top_k")
parser.add_argument("--factor_num", type=int,default=64, help="predictive factors numbers in the model")
parser.add_argument("--layers", nargs='+', default=[64,32,16,8], help="MLP layers. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.")
parser.add_argument("--num_ng", type=int,default=4, help="Number of negative samples for training set")
parser.add_argument("--num_ng_test", type=int,default=100, help="Number of negative samples for test set")
parser.add_argument('-f')

args = parser.parse_args()

final_df = final_df.iloc[:50000]
print(len(final_df))
final_df.head()

def reindex_user_items(ratings):
  user_list = list(ratings['user'].drop_duplicates())
  user2id = {w: i for i, w in enumerate(user_list)}
  
  item_list = list(ratings['song_item'].drop_duplicates())
  item2id = {w: i for i, w in enumerate(item_list)}
  ratings['user'] = ratings['user'].apply(lambda x: user2id[x])
  ratings['song_item'] = ratings['song_item'].apply(lambda x: item2id[x])
  ratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))
  
  return ratings, user2id, item2id, user_list, item_list

indexed_user_item_df, user2id, item2id, user_list, item_list = reindex_user_items(final_df)

user_pool = set(final_df['user'].unique())
item_pool = set(final_df['song_item'].unique())

def leave_one_out(ratings):
  ratings['rank_latest'] = ratings.groupby(['user'])['song_item'].rank(method='first', ascending=True)
  test = ratings.loc[ratings['rank_latest'] == 1]
  train = ratings.loc[ratings['rank_latest'] > 1]
  assert train['user'].nunique()==test['user'].nunique()
  return train[['user', 'song_item', 'rating']], test[['user', 'song_item', 'rating']]

train_df, test_df = leave_one_out(indexed_user_item_df)

def negative_sampling(ratings):
  interact_status = (ratings.groupby('user')['song_item'].apply(set).reset_index().rename(columns={'song_item': 'interacted_items'}))
  interact_status['non_interacted_items'] = interact_status['interacted_items'].apply(lambda x: item_pool - x)
  interact_status['random_non_interacted_samples'] = interact_status['non_interacted_items'].apply(lambda x: random.sample(x, args.num_ng_test))
  return interact_status[['user', 'non_interacted_items', 'random_non_interacted_samples']]

negtaives_df = negative_sampling(indexed_user_item_df)

class Rating_Datset(torch.utils.data.Dataset):
	def __init__(self, user_list, item_list, rating_list):
		super(Rating_Datset, self).__init__()
		self.user_list = user_list
		self.item_list = item_list
		self.rating_list = rating_list

	def __len__(self):
		return len(self.user_list)

	def __getitem__(self, idx):
		user = self.user_list[idx]
		item = self.item_list[idx]
		rating = self.rating_list[idx]
		
		return (
			torch.tensor(user, dtype=torch.long),
			torch.tensor(item, dtype=torch.long),
			torch.tensor(rating, dtype=torch.float)
			)

def get_train_instance(train_df, negtaives_df):
  users, items, ratings = [], [], []
  train_ratings = pd.merge(train_df, negtaives_df[['user', 'non_interacted_items']], on='user')
  train_ratings['negatives'] = train_ratings['non_interacted_items'].apply(lambda x: random.sample(x, args.num_ng))
  for row in train_ratings.itertuples():
    users.append(int(row.user))
    items.append(int(row.song_item))
    ratings.append(float(row.rating))
    for i in range(args.num_ng):
      users.append(int(row.user))
      items.append(int(row.negatives[i]))
      ratings.append(float(0)) 
      
  dataset = Rating_Datset(user_list=users,item_list=items,rating_list=ratings)
  
  return torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)


train_loader = get_train_instance(train_df, negtaives_df)

def get_test_instance(test_df, negtaives_df):
  users, items, ratings = [], [], []
  test_ratings = pd.merge(test_df, negtaives_df[['user', 'non_interacted_items']], on='user')
  for row in test_ratings.itertuples():
    users.append(int(row.user))
    items.append(int(row.song_item))
    ratings.append(float(row.rating))
    for i in getattr(row, 'non_interacted_items'):
      users.append(int(row.user))
      items.append(int(i))
      ratings.append(float(0))
      
  dataset = Rating_Datset(user_list=users,item_list=items,rating_list=ratings)
	
  return torch.utils.data.DataLoader(dataset, batch_size=args.num_ng_test+1, shuffle=False, num_workers=4)

test_loader =get_test_instance(test_df, negtaives_df)

print(type(train_loader))
print(type(test_loader))
num_users = final_df['user'].nunique()+1
num_items = final_df['song_item'].nunique()+1
print(num_users)
print(num_items)

class NeuMF(nn.Module):
    def __init__(self, args, num_users, num_items):
        super(NeuMF, self).__init__()
        self.num_users = num_users
        self.num_items = num_items
        self.factor_num_mf = 128
        self.factor_num_mlp = int(args.layers[0]/2)
        self.layers = args.layers
        self.dropout = args.dropout

        self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)
        self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)

        self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf)
        self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf)

        self.fc_layers = nn.ModuleList()
        for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])):
            self.fc_layers.append(torch.nn.Linear(in_size, out_size))
            self.fc_layers.append(nn.ReLU())

        self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_mf, out_features=1)
        self.logistic = nn.Sigmoid()
        self.init_weight()

    def init_weight(self):
        nn.init.normal_(self.embedding_user_mlp.weight, std=0.01)
        nn.init.normal_(self.embedding_item_mlp.weight, std=0.01)
        nn.init.normal_(self.embedding_user_mf.weight, std=0.01)
        nn.init.normal_(self.embedding_item_mf.weight, std=0.01)
        
        for m in self.fc_layers:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                
        nn.init.xavier_uniform_(self.affine_output.weight)

        for m in self.modules():
            if isinstance(m, nn.Linear) and m.bias is not None:
                m.bias.data.zero_()

    def forward(self, user_indices, item_indices):
        user_embedding_mlp = self.embedding_user_mlp(user_indices)
        item_embedding_mlp = self.embedding_item_mlp(item_indices)

        user_embedding_mf = self.embedding_user_mf(user_indices)
        item_embedding_mf = self.embedding_item_mf(item_indices)

        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector
        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)

        for idx, _ in enumerate(range(len(self.fc_layers))):
            mlp_vector = self.fc_layers[idx](mlp_vector)

        vector = torch.cat([mlp_vector, mf_vector], dim=-1)
        logits = self.affine_output(vector)
        rating = self.logistic(logits)
        return rating.squeeze()

model = NeuMF(args, num_users, num_items)
loss_function = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

print(model)

def hit(ng_item, pred_items): #Hit Ratio
	if ng_item in pred_items:
		return 1
	return 0


def ndcg(ng_item, pred_items): #Normalized Discounted cumulative gain
	if ng_item in pred_items:
		index = pred_items.index(ng_item)
		return np.reciprocal(np.log2(index+2))
	return 0


def metrics(model, test_loader, top_k):
	HR, NDCG = [], []
	recommended_items = dict()

	for user, item, label in test_loader:

		user_id = user[0].item()

		predictions = model(user, item)
		_, indices = torch.topk(predictions, top_k)
		recommends = torch.take(
				item, indices).cpu().numpy().tolist()
		
		recommended_items[user_id] = recommends

		ng_item = item[0].item() 
		HR.append(hit(ng_item, recommends))
		NDCG.append(ndcg(ng_item, recommends))

	return np.mean(HR), np.mean(NDCG), recommended_items

MODEL = 'model_ncf'
best_hr = 0
final_results = []
for epoch in range(1, args.epochs+1):
  model.train()
  start_time = time.time()
  loss_epoch = list()
  
  for user, item, label in train_loader:
    optimizer.zero_grad()
    prediction = model(user, item)
    loss = loss_function(prediction, label)
    loss.backward()
    optimizer.step()
    loss_epoch.append(loss.item())
    #print('loss/Train_loss', loss.item(), epoch)
    
  model.eval()
  print("average loss  for epoch number {}:".format(epoch))
  print(sum(loss_epoch)/len(loss_epoch))
  HR, NDCG, recommended_items = metrics(model, test_loader, args.top_k)
  final_results.append(recommended_items)
  print('Perfomance/HR@10', HR, epoch)
  print('Perfomance/NDCG@10', NDCG, epoch)
  elapsed_time = time.time() - start_time
  print("The time elapse of epoch {:03d}".format(epoch) + " is: " + time.strftime("%H: %M: %S", time.gmtime(elapsed_time)))
  print("HR: {:.3f}\tNDCG: {:.3f}".format(np.mean(HR), np.mean(NDCG)))
  
  if HR > best_hr:
    best_hr, best_ndcg, best_epoch = HR, NDCG, epoch
    torch.save(model, '{}.pth'.format(MODEL))

print("End. Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}".format(best_epoch, best_hr, best_ndcg))

results_to_show = final_results[best_epoch-1]
print(len(results_to_show))

print(results_to_show[0])

stored_results_with_user_id = dict()
for key, value in user2id.items():
  # print(key)
  # print(value)
  items = results_to_show[value]
  stored_results_with_user_id[key] = items

len(stored_results_with_user_id)
stored_results_with_user_id['c22d6e5efe41883b2770c68f6fabf55df5bbbad1']

with open('ncf_model_recommendation.pkl', 'wb') as fb:
    pickle.dump(stored_results_with_user_id, fb)

new_item2_id = dict()
for key, value in item2id.items():
  new_item2_id[value] = key

new_item2_id[0]

final_res = dict()
for key, val in stored_results_with_user_id.items():
  items_id = [(new_item2_id[item]) for item in val]
  final_res[key] = items_id

final_res['c22d6e5efe41883b2770c68f6fabf55df5bbbad1']

with open('ncf_model_recommendation_final.pkl', 'wb') as f:
    pickle.dump(final_res, f)

with open('user2ids.pkl', 'wb') as f:
    pickle.dump(final_res, f)

with open('item2ids.pkl', 'wb') as f:
    pickle.dump(item2id, f)

